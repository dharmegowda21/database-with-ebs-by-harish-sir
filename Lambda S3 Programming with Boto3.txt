

##################################################################################
Help Document: How to write a lambda function using Boto3 which will be triggered
when a file is uploaded to a bucket (lets say, origin bucket). The lambda function will 
process that data. Basically in this case, it just copies that file to another bucket
(destination bucket).

Youtube video link: https://www.youtube.com/watch?v=UOwptW6YKyY 
 

Task 1: Initial Setup
=====================
Launch an Ubuntu EC2

# Ensure Python3 is running.
python3 --version

# If not installed, install python3
sudo apt update
sudo apt install python3.10 -y

# Install boto3. This is the Python SDK library which we will use for coding our 
# lambda function 
pip install boto3  

# You need to install pip, if not already there.
sudo apt install python3-pip -y

# Install awscli
sudo apt install python3-pip -y

# Setup the profile
aws configure
Enter Access Key Id
Enter Secret Access Key
# You can download the AKID/SAK from 
# IAM page > My Security Credentials > AWS IAM Credentials

# See if your profile is updated correctly
aws s3 ls
# If you get list of S3 buckets or no output (if no S3 buckets) then your credentials
# are working fine.

# Create 2 buckets. You can chose your own name.
aws s3 mb s3://origin-bucket-111
aws s3 mb s3://destin-bucket-222


Task 2: Create Lambda function to copy the object
==================================

# Before you create the Lambda function, create a new role MyLambdaRole. Attach following 
# policies: 
     1. AmazonS3FullAccess
     2. CloudWatchFullAccess

# Create a lambda function 's3-copy-function'. Use the above role(MyLambdaRole)for Lambda

# Go to S3. Create event mapping to invoke our lambda function when an object is uploaded 
# in origin-bucket-111.


Task 3: Prepare Lambda code
===========================

nano s3-copy.py

###
import json
import boto3

# boto3 S3 initialization
s3_client = boto3.client("s3")

def lambda_handler(event, context):
   destination_bucket_name = 'destin-bucket-222'

   # event contains all information about uploaded object
   print("Event :", event)

   # Bucket Name where file was uploaded
   source_bucket_name = event['Records'][0]['s3']['bucket']['name']

   # Filename of object (with path)
   file_key_name = event['Records'][0]['s3']['object']['key']

   # Copy Source Object
   copy_source_object = {'Bucket': source_bucket_name, 'Key': file_key_name}

   # S3 copy object operation
   s3_client.copy_object(CopySource=copy_source_object, Bucket=destination_bucket_name, Key=file_key_name)

   return {
       'statusCode': 200,
       'body': json.dumps('Hello from S3 events Lambda!')
   }


# Zip the file so that we can upload it to Lambda
zip s3-copy.zip s3-copy.py

# Register this code (zip file) against the lambda function which we created earlier.
aws lambda update-function-code \
--function-name s3-copy-function \
--zip-file fileb://s3-copy.zip

# Change the handler
aws lambda update-function-configuration \
--function-name s3-copy-function \
--handler s3-copy.lambda_handler

# Create file1.txt. We will upload it into the origin bucket later.
echo "This is the file uploaded to origin bucket" > file1.txt

# List the objects in destin-bucket-222
aws s3 ls s3://destin-bucket-222
# at this point, you won't see any objects

# copy file1.txt to origin bucket
aws s3 cp file1.txt s3://origin-bucket-111/file1.txt 

# list the objects in destin-bucket-222
aws s3 ls s3://destin-bucket-222
# file1.txt will be listed


Task 4 : Cleanup
=================
aws s3 rb s3://origin-bucket-111
aws s3 rb s3://destin-bucket-222

Clean up all the resources you created.




